{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sahibinden.com Scraper (Standard Mode)\n",
    "\n",
    "This notebook safely scrapes rental listings from Sahibinden.com using `undetected-chromedriver` and `BeautifulSoup`.\n",
    "It includes conservative delays and human-like interactions to avoid detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sahibinden.com Scraper for Kurtk\u00f6y Rentals.\n",
    "\n",
    "This script safely scrapes rental listings from Sahibinden.com using undetected-chromedriver\n",
    "and BeautifulSoup. It includes conservative delays and human-like interactions to avoid\n",
    "detection by anti-bot measures.\n",
    "\n",
    "Usage:\n",
    "    Ensure 'sahibinden_urls.txt' exists in the project root with a list of URLs to scrape.\n",
    "    Run the script directly: python scraper_sahibinden.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import undetected_chromedriver as uc\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# ==========================================\n",
    "# Configuration & Constants\n",
    "# ==========================================\n",
    "\n",
    "# Reference coordinates for distance calculations\n",
    "KURTKOY_METRO_COORDS = (40.909444, 29.296111)\n",
    "SABANCI_UNIV_COORDS = (40.890547, 29.378386)\n",
    "BUS_STATION_COORDS = (40.911000, 29.300000)\n",
    "\n",
    "# Scraper Safety Settings\n",
    "MIN_DELAY = 10                # Minimum seconds between requests\n",
    "MAX_DELAY = 25                # Maximum seconds between requests\n",
    "PAGE_LOAD_WAIT = 8            # Seconds to wait for page load to ensure content renders\n",
    "MAX_LISTINGS_PER_SESSION = 15 # Maximum number of listings to scrape in one run\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Initialize an undetected Chrome driver instance with custom options\n",
    "    to mimic a real user browser.\n",
    "    \"\"\"\n",
    "    print(\"Initializing browser session...\")\n",
    "    \n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    \n",
    "    # Use a standard user agent\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    \n",
    "    driver = uc.Chrome(options=options, version_main=None)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"\n",
    "    Perform random scrolling actions to simulate human behavior.\n",
    "    \"\"\"\n",
    "    scroll_pause = random.uniform(0.5, 1.5)\n",
    "    scroll_height = random.randint(300, 600)\n",
    "    \n",
    "    driver.execute_script(f\"window.scrollBy(0, {scroll_height});\")\n",
    "    time.sleep(scroll_pause)\n",
    "    \n",
    "    # Occasionally scroll back up to appear more natural\n",
    "    if random.random() > 0.7:\n",
    "        driver.execute_script(f\"window.scrollBy(0, -{random.randint(50, 150)});\")\n",
    "        time.sleep(random.uniform(0.3, 0.8))\n",
    "\n",
    "def extract_listing_details(driver, url, index, total):\n",
    "    \"\"\"\n",
    "    Extract data from a single listing page.\n",
    "    \n",
    "    Args:\n",
    "        driver: The Selenium/Undetected-Chromedriver instance.\n",
    "        url (str): The URL of the listing to scrape.\n",
    "        index (int): Current listing number.\n",
    "        total (int): Total listings to scrape.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted property details or None if failed.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{index}/{total}] Processing URL: {url[:70]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Navigate to listing\n",
    "        driver.get(url)\n",
    "        time.sleep(PAGE_LOAD_WAIT)\n",
    "        \n",
    "        # Check for manual verification/CAPTCHA\n",
    "        if \"Verify\" in driver.title or \"challenge\" in driver.title.lower():\n",
    "            print(\"\\n  [!] CAPTCHA detected. Please solve it in the browser window.\")\n",
    "            input(\"  Press Enter once resolved...\")\n",
    "        \n",
    "        # Simulate human reading/scrolling\n",
    "        for _ in range(3):\n",
    "            human_like_scroll(driver)\n",
    "        \n",
    "        # Parse content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        details = {\n",
    "            'Listing URL': url,\n",
    "            'Collection Date': datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "        \n",
    "        # 1. Extract Price\n",
    "        price_input = soup.find('input', {'id': 'favoriteClassifiedPrice'})\n",
    "        if price_input and price_input.get('value'):\n",
    "            details['Price'] = price_input['value'].strip()\n",
    "        \n",
    "        # 2. Extract Property Details (Area, Rooms, Age, etc.)\n",
    "        info_list = soup.find('ul', class_='classifiedInfoList')\n",
    "        if info_list:\n",
    "            items = info_list.find_all('li')\n",
    "            for item in items:\n",
    "                try:\n",
    "                    strong = item.find('strong')\n",
    "                    span = item.find('span')\n",
    "                    \n",
    "                    if not strong or not span:\n",
    "                        continue\n",
    "                    \n",
    "                    label = strong.text.strip()\n",
    "                    value = span.text.strip()\n",
    "                    \n",
    "                    if \"\u0130lan Tarihi\" in label:\n",
    "                        details[\"Listing Date\"] = value\n",
    "                    elif \"m\u00b2 (Br\u00fct)\" in label or \"m\u00b2 (Net)\" in label:\n",
    "                        # Prioritize the first area value found\n",
    "                        if \"Area(m2)\" not in details:\n",
    "                            details[\"Area(m2)\"] = value.replace('.', '').strip()\n",
    "                    elif \"Oda Say\u0131s\u0131\" in label:\n",
    "                        details[\"Rooms\"] = value\n",
    "                    elif \"Banyo Say\u0131s\u0131\" in label:\n",
    "                        details[\"Bathrooms\"] = value\n",
    "                    elif \"Bina Ya\u015f\u0131\" in label:\n",
    "                        details[\"Building Age\"] = value\n",
    "                    elif \"E\u015fyal\u0131\" in label:\n",
    "                        details[\"Furnishment\"] = value\n",
    "                    elif \"Kimden\" in label:\n",
    "                        details[\"Listing Type\"] = value\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # 3. Extract Location & Calculate Distances\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='text/javascript')\n",
    "            for script in scripts:\n",
    "                if script.string and 'mapOptions' in script.string:\n",
    "                    lat_match = re.search(r'\"lat\":\\s*([0-9.]+)', script.string)\n",
    "                    lon_match = re.search(r'\"lng\":\\s*([0-9.]+)', script.string)\n",
    "                    \n",
    "                    if lat_match and lon_match:\n",
    "                        lat = float(lat_match.group(1))\n",
    "                        lon = float(lon_match.group(1))\n",
    "                        \n",
    "                        property_coords = (lat, lon)\n",
    "                        # Calculate distances to key points\n",
    "                        details['Distance to Metro (km)'] = round(geodesic(property_coords, KURTKOY_METRO_COORDS).km, 2)\n",
    "                        details['Distance to University (km)'] = round(geodesic(property_coords, SABANCI_UNIV_COORDS).km, 2)\n",
    "                        details['Distance to Bus Station (km)'] = round(geodesic(property_coords, BUS_STATION_COORDS).km, 2)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"    [!] Location extraction failed: {e}\")\n",
    "        \n",
    "        # Summary log\n",
    "        price_log = details.get('Price', 'N/A')\n",
    "        area_log = details.get('Area(m2)', 'N/A')\n",
    "        print(f\"  > Success: Price={price_log}, Area={area_log}m\u00b2\")\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [X] Failed to process listing: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function.\n",
    "    Loads URLs, initializes the scraper, and saves results.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Sahibinden.com Scraper (Standard Mode)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Configuration: Max {MAX_LISTINGS_PER_SESSION} listings, {MIN_DELAY}-{MAX_DELAY}s delay.\")\n",
    "    \n",
    "    # File path for source URLs\n",
    "    # Assuming the file is two directories up based on script location\n",
    "    url_file = \"../../sahibinden_urls.txt\"\n",
    "    print(f\"\\nReading URL list from: {url_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(url_file, 'r') as f:\n",
    "            urls = [line.strip() for line in f if 'sahibinden.com/ilan/' in line.strip()]\n",
    "        print(f\"Found {len(urls)} valid URLs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading URL file: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not urls:\n",
    "        print(\"No URLs to scrape. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Enforce safety limits\n",
    "    if len(urls) > MAX_LISTINGS_PER_SESSION:\n",
    "        print(f\"Limiting scraping to first {MAX_LISTINGS_PER_SESSION} URLs for safety.\")\n",
    "        urls = urls[:MAX_LISTINGS_PER_SESSION]\n",
    "    \n",
    "    # Confirmation\n",
    "    print(f\"Ready to scrape {len(urls)} listings.\")\n",
    "    input(\"Press Enter to start...\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        all_data = []\n",
    "        \n",
    "        for i, url in enumerate(urls, 1):\n",
    "            result = extract_listing_details(driver, url, i, len(urls))\n",
    "            if result:\n",
    "                all_data.append(result)\n",
    "            \n",
    "            # Respectful delay between requests\n",
    "            if i < len(urls):\n",
    "                delay = random.uniform(MIN_DELAY, MAX_DELAY)\n",
    "                print(f\"  Waiting {delay:.1f} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        # Save Results\n",
    "        if all_data:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_path = f\"../../data/raw/sahibinden/sahibinden_safe_scrape_{timestamp}.xlsx\"\n",
    "            \n",
    "            df.to_excel(output_path, index=False)\n",
    "            \n",
    "            print(\"\\n\" + \"-\" * 60)\n",
    "            print(\"Scraping Completed Successfully\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Total Listings: {len(all_data)}\")\n",
    "            print(f\"Output File:    {output_path}\")\n",
    "        else:\n",
    "            print(\"\\nScraping finished but no data was collected.\")\n",
    "            \n",
    "    finally:\n",
    "        print(\"\\nShutting down browser...\")\n",
    "        time.sleep(3)\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}